{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64186cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd      \n",
    "import numpy as np       \n",
    "import sklearn                             # importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85bd0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"usage_train.csv\")      # reading the training data\n",
    "train.fillna(0,inplace = True)             # filling the missing values with 0's because \n",
    "                                           # most of the data missing is of products \n",
    "                                           # purchased and signup dates\n",
    "cols_list = ['user_activity_var_1','user_activity_var_2','user_activity_var_3','user_activity_var_4','user_activity_var_5','user_activity_var_6','user_activity_var_7','user_activity_var_8','user_activity_var_9','user_activity_var_10','user_activity_var_11','user_activity_var_12']\n",
    "\n",
    "                                                 # feature engineering \n",
    "                                                 # summing the user activities to get a \n",
    "                                                 # new and effective feature sum of user \n",
    "                                                 # activities\n",
    "train['sum_user_activity'] = train[cols_list].sum(axis=1)\n",
    "\n",
    "feature_cols = cols_list + ['campaign_var_1','campaign_var_2', 'sum_user_activity']\n",
    "                                                 # all the features except dates\n",
    "X = train[feature_cols]                          # input training set\n",
    "y = train.buy                                    # output training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89816111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=200,stop=2000,num=10)]\n",
    "max_features = ['auto','sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10,110,num=11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split= [2,5,10]                    # setting up the RandomizedSearch with the \n",
    "min_samples_leaf = [1,2,4]                     # possiblities of the search\n",
    "bootstrap = [True,False]\n",
    "\n",
    "random_grid = {'n_estimators':n_estimators,\n",
    "              'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}              # setting up in the form of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4ae78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier              \n",
    "rf = RandomForestClassifier()                       # creating a variable for the classifier\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = \n",
    "                               random_grid, n_iter = 20,cv = 2,verbose=2,\n",
    "                              random_state=42, n_jobs=-1)\n",
    "                                                    # creating a variable for the \n",
    "                                                    # randomized search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fbbfa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"usage_test.csv\")                # setting up the test data and \n",
    "test.fillna(0,inplace = True)                       # feature engineering and pre processing \n",
    "test['sum_user_activity'] = test[cols_list].sum(axis=1)    \n",
    "X_test = test[feature_cols]             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3269b267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n"
     ]
    }
   ],
   "source": [
    "rf_random.fit(X,y)                                  # fitting the data\n",
    "y_pred_rForest = rf_random.predict(X_test)          # predicting on test set \n",
    "y_pred_rForest = pd.DataFrame(y_pred_rForest)       # converting prediciton into a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rForest = pd.concat([test['id'],y_pred_rForest],join='outer',axis=1) # connecting \n",
    "y_pred_rForest.columns = ['id','buy']               # with the index and naming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b566047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rForest.to_csv('y_pred',index = False)                   # writing the csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a70f3d",
   "metadata": {},
   "source": [
    "Approach\n",
    "\n",
    "Brief:\n",
    "\n",
    "Trianed and Tested on variety of algorithms and concluded with the randomised search random\n",
    "forest classifier on the basis of score\n",
    "\n",
    "Data Preprocessing step include filling up the empty products purchased column with 0's \n",
    "because if no data is recorded it would most probably no products bought\n",
    "\n",
    "Feature Engineering step comprises of the process of adding another column with sum of all \n",
    "user activities as the  magnitude of the user activity/engagement influences most with the sale probability.\n",
    "\n",
    "\n",
    "Final model \n",
    "\n",
    "It's a time taking model based on the hypertuning of parameters take long time. \n",
    "The logistic classification, knn with variation of k's, Naive Bayes classifier algorithms, decision trees\n",
    "are used but were not effective compared to the random forest classifier.\n",
    "\n",
    "so for further improvement to the model, the hypertuning of parameteres was a great boost for the improvement of model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dce8c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
